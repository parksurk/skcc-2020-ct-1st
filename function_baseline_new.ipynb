{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "function_baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSPisf0Kftcr",
        "colab_type": "text"
      },
      "source": [
        "## <strong>0. 시작하기</strong>\n",
        "\n",
        "이 노트북은 이제 막 데이터 분석에 관심있는 구성원을 위하여 작성되었습니다.\n",
        "\n",
        "<strong>파라미터의 변경 입력</strong>만으로 <strong>예측 모델의 성능을 개선</strong>해 볼 수 있습니다. 다음의 순서대로 진행해 보세요.\n",
        "\n",
        "1. TendorFlow 2.x 선택\n",
        "\n",
        "2. 실행함수 로딩\n",
        "\n",
        "3. 나만의 모델 학습 및 성능 확인 \n",
        "\n",
        "4. 제출용 파일 생성 \n",
        "\n",
        "### <strong>시작하기 전에, GPU를 사용하도록 설정 하셨나요?.</strong>\n",
        "\n",
        "GPU를 사용하도록 설정해 두면 모델의 학습시간이 단축됩니다.\n",
        "\n",
        "- 화면 상단의 '<strong>런타임</strong>' 메뉴에서 '<strong>런타임 유형 변경</strong>'을 클릭하세요.\n",
        "- 런타임 유형은 '<strong>Python 3</strong>'으로, 하드웨어 가속기는 '<strong>GPU</strong>'로 선택하고 '<strong>저장</strong>'을 클릭하세요.\n",
        "\n",
        "\n",
        "\n",
        "### <strong>이제, 데이터세트를 준비해 주세요.</strong>\n",
        "- 경연 페이지에서 <strong>train.csv</strong>(학습 및 테스트용)와 <strong>predict_input.csv</strong>(문제용)을 다운로드 하세요.\n",
        "- 이 노트북 왼쪽 영역에서 '<strong>폴더</strong>' 아이콘을 클릭하세요.\n",
        "- '<strong>업로드</strong>'를 클릭해서 <strong>train.csv</strong>와 <strong>predict_input.csv</strong> 파일을 업로드하세요.\n",
        "- 잠시 후, 왼쪽 영역의 파일목록 하단에 업로드 진행 상태가 표시됩니다. 완료가 될때까지 기다려 주세요.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng5T3OAAkkG4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## <strong>1. TensorFlow 2.x 선택</strong>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "이 노트북에 포함된 코드는 TensorFlow 2.0을 기준으로 작성되었습니다.\n",
        "\n",
        "Google Colab은 기본적으로 TensorFlow 1.x 환경이기 때문에 TensorFlow 2.x 환경으로 변경하기 위해서는 다음과 같이 별도의 명령을 입력해야 합니다. \n",
        "\n",
        "> <strong>다음의 코드를 실행합니다.</strong> (코드 영역을 클릭하면 왼쪽에 실행 버튼이 나타납니다.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4s76n4OgZFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C5oSb75uQ3y",
        "colab_type": "text"
      },
      "source": [
        "코드 아래에 실행 결과가 '<strong>TensorFlow 2.x selected.</strong>'로 표시되면 정상적으로 실행된 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsw6kbXwg-GB",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## <strong>2. 실행함수 로딩</strong>\n",
        "\n",
        "다음의 코드는 여러가지 기능을 하는 함수가 포함되어 있습니다.\n",
        "\n",
        "- 학습 및 테스트용 데이터세트를 읽어오기\n",
        "\n",
        "- 모델을 학습, 저장하고 성능을 표시\n",
        "\n",
        "- 저장된 모델과 문제용 데이터세트를 가져오기 \n",
        "\n",
        "- 독성을 예측하고 제출용 데이터세트를 생성\n",
        "\n",
        "코드의 내용을 자세하게 이해할 수 없더라도 일단 실행해 봅시다. \n",
        "\n",
        "> <strong>다음의 코드를 실행합니다.</strong> (코드 영역을 클릭하면 왼쪽에 실행 버튼이 나타납니다.)\n",
        "\n",
        "코드 실행 후 코드의 맨 아래로 화면을 스크롤하면 잠시 후 실행 결과가 순차적으로 나타납니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARDo6biGg1fe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf     \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from tensorflow.keras import Input\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "def train(\n",
        "    epoch           = 20,\n",
        "    batch_size      = 10,\n",
        "    learning_rate   = 1e-5,\n",
        "    conv_block_info = [32,3,64,3,64,3],\n",
        "    fc_block_info   = [128],\n",
        "    drop_out_rate   = 0.2 \n",
        "):\n",
        "\n",
        "    model_save_path = './Train_Result'           \n",
        "    try:\n",
        "      if not(os.path.isdir(model_save_path)):\n",
        "        os.makedirs(os.path.join(model_save_path))\n",
        "\n",
        "    except OSError as e:\n",
        "      if e.errno != errno.EEXIST:\n",
        "        print('학습 결과를 저장할 폴더 생성에 실패하였습니다.')\n",
        "        raise\n",
        "\n",
        "\n",
        "    train_data_path = './train.csv'       \n",
        "    if not os.path.isfile(train_data_path):\n",
        "        print('학습데이터 파일이 경로에 존재하지 않습니다.')\n",
        "\n",
        "    dataset         = pd.read_csv(train_data_path)\n",
        "    print(dataset.head())\n",
        "\n",
        "    feature         = dataset.iloc[:,1:1025].values.astype(float)     \n",
        "    label           = dataset['label'].values\n",
        "    nrow, ncol      = feature.shape\n",
        "\n",
        "    input_data_size = 1024\n",
        "    feature         = tf.reshape(feature, (-1, input_data_size, 1, 1))   \n",
        "    input_shape     = (input_data_size, 1, 1)\n",
        "\n",
        "    full_dataset    = tf.data.Dataset.from_tensor_slices((feature , label))\n",
        "\n",
        "    train_size      = int(0.9 * nrow)         \n",
        "    train_dataset   = full_dataset.take(train_size)\n",
        "    val_dataset     = full_dataset.skip(train_size)\n",
        "\n",
        "    train_dataset   = train_dataset.shuffle(train_size).batch(batch_size).repeat()\n",
        "    val_dataset     = val_dataset.batch(batch_size)\n",
        "\n",
        "    val_label = []\n",
        "    for data, label in val_dataset:\n",
        "      val_label.extend(label.numpy().tolist())\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Input(shape=input_shape))\n",
        "\n",
        "    for i in range(0, len(conv_block_info), 2):\n",
        "      filters_ = int(conv_block_info[i])\n",
        "      kernel_size_ = int(conv_block_info[i+1])\n",
        "\n",
        "      model.add(Conv2D(filters=filters_, kernel_size=(kernel_size_, 1), strides=(1, 1), padding='same'))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(MaxPool2D((2, 1), padding='same'))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    for fc_unit in fc_block_info:\n",
        "      fc_unit_ = int(fc_unit)\n",
        "\n",
        "      model.add(Dense(fc_unit_, activation='relu'))\n",
        "      model.add(Dropout(drop_out_rate))\n",
        "\n",
        "    model.add(Dense(2, activation='softmax'))\n",
        "    model.summary()\n",
        "\n",
        "    adam = tf.keras.optimizers.Adam(learning_rate = learning_rate )\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "    model.compile(optimizer = adam, \n",
        "                  loss = loss_fn,\n",
        "                  metrics = ['accuracy'])\n",
        "\n",
        "    model.fit( train_dataset, epochs = epoch, steps_per_epoch = train_size//batch_size)\n",
        "\n",
        "    model.save( model_save_path, save_format='tf' )\n",
        "\n",
        "    val_pred = model.predict( val_dataset )\n",
        "    val_pred = np.argmax(val_pred,1)\n",
        "\n",
        "    print('\\n-----[Evaluation]-----')\n",
        "    print(classification_report(val_label, val_pred, digits=4))\n",
        "    print('\\n-----[Confusion Matrix]-----')\n",
        "    print(confusion_matrix(val_label, val_pred))\n",
        "\n",
        "def predict():\n",
        "\n",
        "    model_save_path = './Train_Result'           \n",
        "    test_data_path = './predict_input.csv'       \n",
        "\n",
        "    batch_size              = 10\n",
        "    input_data_size         = 1024\n",
        "\n",
        "    if not os.path.isfile(test_data_path):\n",
        "        print('학습데이터 파일이 경로에 존재하지 않습니다.')\n",
        "\n",
        "    dataset         = pd.read_csv(test_data_path)\n",
        "    print(dataset.head())\n",
        "\n",
        "    feature         = dataset.iloc[:,1:1025].values.astype(float)     \n",
        "    nrow, ncol      = feature.shape\n",
        "\n",
        "    feature         = tf.reshape( feature , (-1 , input_data_size , 1 , 1) )   \n",
        "\n",
        "    test_dataset    = tf.data.Dataset.from_tensor_slices( feature )\n",
        "    test_dataset    = test_dataset.batch( batch_size )\n",
        "\n",
        "    model = tf.keras.models.load_model( model_save_path ) \n",
        "\n",
        "    y_ = model.predict( test_dataset )\n",
        "    model.summary()\n",
        "\n",
        "    print(y_)\n",
        "    y_ = np.argmax(y_,1)\n",
        "\n",
        "    result = dataset[['SMILES']].copy()\n",
        "    result['label'] = y_\n",
        "    result.to_csv('./predict_output.csv', index=False)\n",
        "\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzz1sa96vy2W",
        "colab_type": "text"
      },
      "source": [
        "위 코드가 정상적으로 실행되면 실행 결과에는 Default 파라미터를 이용한 모델의 학습과정과 성능 평가 결과가 표시됩니다.\n",
        "\n",
        "- 실행 결과의 제일 마지막 부분에서 1과 0의 <strong>F1 Score</strong>를 확인할 수 있습니다.\n",
        "\n",
        "- 1은 독성이 없음을, 0은 독성이 있음을 의미합니다. \n",
        "\n",
        "- 1(독성 없음)의 F1 Score는 이번 경연에서 순위를 결정짓는 중요한 점수이며 <strong>'1.0'에 가까울수록 높은 성능</strong>을 의미합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULFr34JHg7mw",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## <strong>3. 나만의 모델 학습 및 성능 확인</strong>\n",
        "\n",
        "위 코드의 실행 결과 마지막 부분에서  <strong>[Evaluation]</strong>과 <strong>[Confusion Matrix]</strong>가 표시되면 정상적으로 실행된 것입니다. [Evaluation]의 내용에서 1(독성 없음)의 F1 Score는 얼마였나요?\n",
        "\n",
        "이제부터 여러분의 생각대로 파라미터를 조정해서 모델을 학습시키고 F1 Score가 어떻게 변하는지 확인하는 방법을 설명하겠습니다. \n",
        "\n",
        "우리가 실행시킬 <strong>train()</strong>함수는 총 6개의 파라미터를 지정할 수 있습니다.\n",
        "\n",
        "- train( <strong>epoch</strong> = 20, <strong>batch_size</strong> = 10, <strong>learning_rate</strong> = 1e-5, <strong>conv_block_info</strong> = [32,3,64,3,64,3], <strong>fc_block_info</strong> = [128], <strong>drop_out_rate</strong> = 0.2 )\n",
        "\n",
        "\n",
        "각 파라미터의 Default Value와 의미는 다음과 같습니다.\n",
        "- <strong>epoch</strong> = 20 : 학습용 데이터세트를 이용한 학습의 횟수 = 20번\n",
        "- <strong>batch_size</strong> = 10 : 한번의 Batch마다 주는 샘플 데이터의 갯수 = 10개\n",
        "- <strong>learning_rate</strong> = 1e-5 : 모델을 학습시키는 데 사용되는 기울기의 보폭(학습율) = 0.00001\n",
        "- <strong>conv_block_info</strong> = [32,3,64,3,64,3] : 3개로 구성된 Convolution Layer 각각의 Filter 수 및 Kernal의 크기\n",
        "- <strong>fc_block_info</strong> = [128] : Fully Connected Layer의 Unit 수 = 128개\n",
        "- <strong>drop_out_rate</strong> = 0.2 : Overfitting 방지를 위한 Dropout 비율 = 20%\n",
        "\n",
        "\n",
        "이제 학습의 횟수만 5번으로 변경하여 함수를 실행해 봅시다. \n",
        "\n",
        "> <strong>다음의 코드를 실행해 봅시다.</strong>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yK6papcxfG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train( epoch = 5 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeY4u8CLxjdc",
        "colab_type": "text"
      },
      "source": [
        "F1 Score이 달라졌나요? 처음보다 올라갔나요?\n",
        "\n",
        "그럼 epoch와 batch_size를 함께 바꿔보도록 합시다.\n",
        "\n",
        "다음은 학습 횟수는 5번으로 Batch size는 20개로 변경한 코드입니다.\n",
        "\n",
        "> <strong>다음의 코드를 실행해 봅시다.</strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhhJxgmi1pG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train( epoch = 5, batch_size = 20 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80R392gm1xf8",
        "colab_type": "text"
      },
      "source": [
        "F1 Score가 또 변했죠? \n",
        "\n",
        "train( )은 위와 같이 원하는 파라미터의 Value를 변경하여 모델을 학습하고 성능을 평가할 수 있는 함수입니다. \n",
        "\n",
        "자, 이제 본격적으로 나만의 모델을 위한 작업을 해 봅시다.\n",
        "\n",
        "> <strong>다음의 코드에서 6가지 파라미터의 Value를 원하는 값으로 변경한 후 실행해 봅시다.</strong>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFZbTuSahBzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train( epoch = 20, batch_size = 10, learning_rate = 1e-5, conv_block_info = [32,3,64,3,64,3], fc_block_info = [128], drop_out_rate = 0.2 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNweyXKBxE0D",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "어떻습니까? F1 Score가 좋아지고 있나요? 아니라면 다른 값을 넣어서 다시 실행해 보세요. 원하는 F1 Score를 만들기 위해서는 다양한 Value 값을 이용하여 위 코드를 실행해 봐야 합니다.  \n",
        "\n",
        "F1 Score를 높이기 위해서 무작위로 수치를 넣는것 보다 CNN(Convolutional Neural Networks)을 조금 공부하고 도전해 보는 것도 괜찮겠죠?\n",
        "\n",
        "F1 Score가 원하는 만큼 개선이 되었다면 이제 다음의 <strong>'4. 제출용 파일 생성'</strong>으로 넘어가 봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hJgHh3GhdGd",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "## <strong>4. 제출용 파일 생성</strong>\n",
        "\n",
        "여기까지 오셨다면 마음에 드는 F1 Score를 찾으신거죠? 하지만 이 F1 Score는 여러분에게 제공된 학습 및 테스트용 데이터세트를 기준으로 표시된 결과입니다. 여러분이 제출하는 파일의 F1 Score는 별도로 계산되며 그 결과는 경연페이지의 리더보드에서 확인할 수 있습니다.\n",
        "\n",
        "이제 위에서 개발한 모델과 문제용 데이터세트를 이용해서 독성의 유무를 예측하고 제출할 답안용 데이터세트를 만들어 봅시다.\n",
        "\n",
        "> <strong>아래의 'predict()' 코드를 실행합니다.</strong> 별도의 파라미터 지정은 필요 없습니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtOZvSWSheJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkOvlUUPCdxw",
        "colab_type": "text"
      },
      "source": [
        "- 잠시 후 왼쪽 영역에 '<strong>predict_output.csv</strong>' 파일이 생긴것을 확인할 수 있습니다. \n",
        "\n",
        "- 안보이면 '<strong>새로고침</strong>'을 클릭해 보세요.\n",
        "- 파일에서 마우스 오른쪽 버튼을 클릭한 후 '<strong>다운로드</strong>'를 클릭하세요.\n",
        "- 원하는 디렉토리에 저장하면 끝.\n",
        "\n",
        "제출 방법을 참고해서 'predict_output.csv' 파일을 제출해 주세요. \n",
        "\n",
        "수고하셨습니다."
      ]
    }
  ]
}